{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.read.option('header','true')\\\n",
    ".option('index','false')\\\n",
    ".option('inferSchema','true')\\\n",
    ".csv(\"file:///F:\\MiNI IAD\\Sem 1\\Big Data\\GitHub-Repos-BigData\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'diskUsage', 'forkCount', 'isFork', 'Python', 'unclosed_issues', 'assign_', 'assign_bio', 'stargazer_', 'milestone', 'milestone_closed', 'release_', 'Ruby', 'Shell', 'HTML', 'JavaScript', 'Makefile', 'CSS', 'C++', 'C', 'CMake', 'Java', 'RepoAge', 'RepoLife', 'label', 'languageCounter', 'popularLanguageCounter', 'hasLanguage', 'description len', 'has description', 'has issue', 'stargazer100', 'stargazer non zero', 'has milestone']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.__class__\n",
    "df = df.drop('label', '_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(diskUsage=3.2857896311685453, forkCount=41, isFork=0, Python=1.0, unclosed_issues=1.0, assign_=51.0, assign_bio=0.0, stargazer_=100.0, milestone=0.0, milestone_closed=0.0, release_=0.0, Ruby=0.0, Shell=0.0, HTML=0.0, JavaScript=0.0, Makefile=0.0, CSS=0.0, C++=0.0, C=0.0, CMake=0.0, Java=0.0, RepoAge=531, RepoLife=525, languageCounter=1, popularLanguageCounter=1, hasLanguage=0, description len=64, has description=0, has issue=0, stargazer100=0, stargazer non zero=0, has milestone=0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[diskUsage: double, forkCount: int, isFork: int, Python: double, unclosed_issues: double, assign_: double, assign_bio: double, stargazer_: double, milestone: double, milestone_closed: double, release_: double, Ruby: double, Shell: double, HTML: double, JavaScript: double, Makefile: double, CSS: double, C++: double, C: double, CMake: double, Java: double, RepoAge: int, RepoLife: int, languageCounter: int, popularLanguageCounter: int, hasLanguage: int, description len: int, has description: int, has issue: int, stargazer100: int, stargazer non zero: int, has milestone: int]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "features.remove(\"RepoAge\")\n",
    "data = df.select(col(\"RepoAge\").alias(\"label\"), *features)\n",
    "(training, test) = data.randomSplit([.7, .3])\n",
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 695.215\n",
      "MSE: 483323.481\n",
      "MAE: 553.683\n",
      "r2: 0.241\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(prediction)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
