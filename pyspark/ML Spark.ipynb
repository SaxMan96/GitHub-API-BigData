{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at C:\\Users\\Mateusz\\Miniconda3\\envs\\spark-env\\lib\\site-packages\\IPython\\utils\\py3compat.py:188 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4154298628d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Hadoop\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mE:\\Hadoop\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at C:\\Users\\Mateusz\\Miniconda3\\envs\\spark-env\\lib\\site-packages\\IPython\\utils\\py3compat.py:188 "
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.read.option('header','true')\\\n",
    ".option('index','false')\\\n",
    ".option('inferSchema','true')\\\n",
    ".csv(\"file:///F:\\MiNI IAD\\Sem 1\\Big Data\\GitHub-Repos-BigData\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.__class__\n",
    "df = df.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(diskUsage=2.531667508315967, forkCount=0, squashMergeAllowed=0, isArchived=0, isFork=0, Ruby=0.8642358417377812, assign_=3.0, stargazer_=2.0, milestone=0.0, Python=0.0, Shell=0.0, HTML=0.0, JavaScript=0.0, Makefile=0.0, C++=0.0, C=0.0, Java=0.0, CSS=0.0, RepoAge=3057, RepoLife=283, label=0, languageCounter=2, popularLanguageCounter=1, hasLanguage=0, description_len=44, has_description=0, has_issue=0, stargazer_non_zero=0, has_milestone=0, has_release=0, contributed=0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[diskUsage: double, forkCount: int, squashMergeAllowed: int, isArchived: int, isFork: int, Ruby: double, assign_: double, stargazer_: double, milestone: double, Python: double, Shell: double, HTML: double, JavaScript: double, Makefile: double, C++: double, C: double, Java: double, CSS: double, RepoAge: int, RepoLife: int, label: int, languageCounter: int, popularLanguageCounter: int, hasLanguage: int, description_len: int, has_description: int, has_issue: int, stargazer_non_zero: int, has_milestone: int, has_release: int, contributed: int]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "features.remove(\"label\")\n",
    "data = df.select(col(\"label\"), *features)\n",
    "(training, test) = data.randomSplit([.7, .3])\n",
    "vectorAssembler = VectorAssembler(inputCols=features,\n",
    "                                  outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\",\n",
    "                                outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.255\n",
      "MSE: 0.065\n",
      "MAE: 0.163\n",
      "r2: 0.357\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"label\",\n",
    "                           predictionCol=\"prediction\",\n",
    "                           metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(prediction)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-33f8c64e6fec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "row['features'].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at C:\\Users\\Mateusz\\Miniconda3\\envs\\spark-env\\lib\\site-packages\\IPython\\utils\\py3compat.py:188 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4154298628d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Hadoop\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mE:\\Hadoop\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at C:\\Users\\Mateusz\\Miniconda3\\envs\\spark-env\\lib\\site-packages\\IPython\\utils\\py3compat.py:188 "
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.12834224598930483\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 3 with 13 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "# data = MLUtils.loadLibSVMFile(sc, \"file:///F:\\MiNI IAD\\Sem 1\\Big Data\\GitHub-Repos-BigData\\data.csv\",multiclass=True, numFeatures=31).collect()\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "def parse_points(df):\n",
    "    \"\"\"Converts a DataFrame of comma separated unicode strings into a DataFrame of `LabeledPoints`.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame where each row is a comma separated unicode string. The first element in the string\n",
    "            is the label and the remaining elements are the features.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Each row is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features. To convert an RDD to a DataFrame, simply call toDF().\n",
    "    \"\"\"\n",
    "    token = df.split(\",\")\n",
    "    #     print(token.__class__)\n",
    "    label = float(token[0])\n",
    "    features = token[1:]\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "\n",
    "raw_data_df = sc.textFile(\n",
    "    \"file:///F:\\MiNI IAD\\Sem 1\\Big Data\\GitHub-Repos-BigData\\data.csv\")\n",
    "\n",
    "features = raw_data_df.take(1)\n",
    "tagsheader = raw_data_df.first()\n",
    "header = sc.parallelize([tagsheader])\n",
    "raw_data_df = raw_data_df.subtract(header)\n",
    "\n",
    "parsed_points_df = raw_data_df.map(lambda x: parse_points(x))\n",
    "\n",
    "first_point_features = parsed_points_df.first().features\n",
    "first_point_label = parsed_points_df.first().label\n",
    "\n",
    "(trainingData, testData) = parsed_points_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData,\n",
    "                                     numClasses=2,\n",
    "                                     categoricalFeaturesInfo={},\n",
    "                                     impurity='gini',\n",
    "                                     maxDepth=3,\n",
    "                                     maxBins=32)\n",
    "\n",
    "# # Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count(\n",
    ") / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model)\n",
    "\n",
    "# # Save and load model\n",
    "# model.save(sc, \"../myDecisionTreeClassificationModel\")\n",
    "# sameModel = DecisionTreeModel.load(sc, \"../myDecisionTreeClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"diskUsage\", \"forkCount\", \"squashMergeAllowed\", \"isArchived\",\n",
    "    \"isFork\", \"C\", \"Shell\", \"C++\", \"assign_\", \"stargazer_\", \"milestone\",\n",
    "    \"Ruby\", \"Makefile\", \"Python\", \"JavaScript\", \"HTML\", \"CSS\", \"Java\",\n",
    "    \"RepoAge\", \"RepoLife\", \"languageCounter\", \"popularLanguageCounter\",\n",
    "    \"hasLanguage\", \"description_len\", \"has_description\", \"has_issue\",\n",
    "    \"stargazer_non_zero\", \"has_milestone\", \"has_release\", \"contributed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 13 nodes\n",
      "  If (forkCount <= 16.5)\n",
      "   If (Python <= 0.0012239585494989368)\n",
      "    Predict: 0.0\n",
      "   Else (Python > 0.0012239585494989368)\n",
      "    If (feature 0 <= 185257.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 185257.5)\n",
      "     Predict: 1.0\n",
      "  Else (forkCount > 16.5)\n",
      "   If (has_milestone <= 0.5)\n",
      "    If (assign_ <= 20.5)\n",
      "     Predict: 0.0\n",
      "    Else (assign_ > 20.5)\n",
      "     Predict: 1.0\n",
      "   Else (has_milestone > 0.5)\n",
      "    If (Python <= 0.9998723173667664)\n",
      "     Predict: 1.0\n",
      "    Else (Python > 0.9998723173667664)\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_str = model.toDebugString()\n",
    "for i in range(len(features)-1,0,-1):\n",
    "    f = \"feature \"+str(i)\n",
    "    tree_str = tree_str.replace(f,features[i])\n",
    "print(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Success !\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def parse(lines):\n",
    "    block = []\n",
    "    while lines:\n",
    "\n",
    "        if lines[0].startswith('If'):\n",
    "            bl = ' '.join(lines.pop(0).split()[1:]).replace('(', '').replace(\n",
    "                ')', '')\n",
    "            block.append({'name': bl, 'children': parse(lines)})\n",
    "\n",
    "            if lines[0].startswith('Else'):\n",
    "                be = ' '.join(lines.pop(0).split()[1:]).replace('(',\n",
    "                                                                '').replace(\n",
    "                                                                    ')', '')\n",
    "                block.append({'name': be, 'children': parse(lines)})\n",
    "        elif not lines[0].startswith(('If', 'Else')):\n",
    "            block2 = lines.pop(0)\n",
    "            block.append({'name': block2})\n",
    "        else:\n",
    "            break\n",
    "    return block\n",
    "\n",
    "\n",
    "def tree_json(tree):\n",
    "    data = []\n",
    "    for line in tree.splitlines():\n",
    "        if line.strip():\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "        else:\n",
    "            break\n",
    "        if not line: break\n",
    "    res = []\n",
    "    res.append({'name': 'Root', 'children': parse(data[1:])})\n",
    "    with open('structure.json', 'w+') as outfile:\n",
    "        json.dump(res[0], outfile)\n",
    "    print('Conversion Success !')\n",
    "\n",
    "tree_json(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testData.map(lambda p: p.features))\n",
    "labels_and_preds = testData.map(lambda p: p.label).zip(predictions)\n",
    "test_accuracy = labels_and_preds.filter(lambda xy: (xy[1] == xy[0])).count(\n",
    ") / float(testData.count())\n",
    "print(\"Test accuracy is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://vanya.jp.net/vtree/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
